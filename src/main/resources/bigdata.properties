#Kafka
#测试环境
#生产环境地址
#metadata.broker.list=192.168.0.113:9092,192.168.0.114:9092,192.168.0.116:9092
metadata.broker.list=${maven.metadata.broker.list}

serializer.class=kafka.serializer.StringEncoder
key.serializer.class=kafka.serializer.StringEncoder
request.required.acks=0
producer.type=async
compression.codec=gzip
batch.num.messages=200

#HBase
#测试环境
#线上地址
#hbase.zookeeper.quorum=192.168.0.113,192.168.0.114,192.168.0.115
hbase.zookeeper.quorum=${maven.hbase.zookeeper.quorum}
hbase.zookeeper.property.clientPort=2181

#HDFS
#fs.defaultFS=hdfs://nameservice1
#dfs.nameservices=nameservice1
#ha.zookeeper.quorum=myd3:2181,myd4:2181,myd5:2181
#dfs.ha.namenodes.nameservice1=namenode333,namenode336
#nn1=dfs.namenode.rpc-address.nameservice1.namenode333
#nn2=dfs.namenode.rpc-address.nameservice1.namenode336
#dfs.namenode.rpc-address.nameservice1.nn1=myd1:8020
#dfs.namenode.rpc-address.nameservice1.nn2=myd3:8020
#dfs.client.failover.proxy.provider.nameservice1=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider

fs.defaultFS=${fs.defaultFS}
dfs.nameservices=${dfs.nameservices}
ha.zookeeper.quorum=${ha.zookeeper.quorum}
dfs.ha.namenodes.nameservice1=${dfs.ha.namenodes.nameservice1}
nn1=${nn1}
nn2=${nn2}
dfs.namenode.rpc-address.nameservice1.nn1=${dfs.namenode.rpc-address.nameservice1.nn1}
dfs.namenode.rpc-address.nameservice1.nn2=${dfs.namenode.rpc-address.nameservice1.nn2}
dfs.client.failover.proxy.provider.nameservice1=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider